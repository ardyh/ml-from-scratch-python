{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import seed\n",
    "from random import random\n",
    "from math import exp\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_iris\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import OneHotEncoder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network:\n",
    "  #constructor\n",
    "  def __init__(self, n_inputs, n_hidden, n_outputs=3, bias=1):\n",
    "    self.n_inputs = n_inputs # number of input unit\n",
    "    self.n_hidden = n_hidden # number of hidden unit\n",
    "    self.n_outputs = n_outputs # number of output unit\n",
    "    self.bias = bias # bias parameter\n",
    "    \n",
    "    # parameters of weight on input to hidden layer \n",
    "    self.weights_ItoH = np.random.uniform(-1, 1, (n_inputs+1, n_hidden)) \n",
    "    self.dweights_ItoH = np.zeros((n_inputs+1, n_hidden))\n",
    "    \n",
    "    # parameters of weight on hidden to output layer \n",
    "    self.weights_HtoO = np.random.uniform(-1, 1, (n_hidden+1, n_outputs))\n",
    "    self.dweights_HtoO = np.zeros((n_hidden+1, n_outputs))\n",
    "    \n",
    "    # output value and error of hidden layer\n",
    "    self.pre_activation_H = np.zeros(n_hidden)\n",
    "    self.post_activation_H = np.zeros(n_hidden)\n",
    "    self.error_H = np.zeros(n_hidden)\n",
    "    \n",
    "    # output value and error of output layer\n",
    "    self.pre_activation_O = np.zeros(n_outputs)\n",
    "    self.post_activation_O = np.zeros(n_outputs)\n",
    "    self.error_O = np.zeros(n_outputs)\n",
    "  \n",
    "  # Net calculation method\n",
    "  ## Calculate net for an input\n",
    "  def calculate_net_ItoH(self, sample, node):\n",
    "    input_plus_bias = np.append(self.data[sample,:], self.bias)\n",
    "    return np.dot(input_plus_bias, self.weights_ItoH[:, node])\n",
    "  ## Calculate net for a hidden unit\n",
    "  def calculate_net_HtoO(self, node):\n",
    "    hidden_plus_bias =  np.append(self.post_activation_H, self.bias)\n",
    "    return np.dot(hidden_plus_bias, self.weights_HtoO[:, node])\n",
    "\n",
    "  # activation function\n",
    "  def activation(self, x):\n",
    "  \treturn 1.0/(1.0 + np.exp(-x))\n",
    "\n",
    "  def one_hot_encode(self, target):\n",
    "    encoder = OneHotEncoder(sparse=False)\n",
    "    new_target = target.reshape(len(target), 1)\n",
    "    target_encode = encoder.fit_transform(new_target)\n",
    "    return target_encode\n",
    "\n",
    "  #fit the network to the data\n",
    "  def fit(self, data, target, epoch_limit=100, mini_batch_limit=10):\n",
    "    self.data = data\n",
    "    self.target = self.one_hot_encode(target)\n",
    "    self.epoch_limit = epoch_limit\n",
    "\n",
    "    len_data = len(data)\n",
    "\n",
    "    # iterate each epoch\n",
    "    for epoch in range(epoch_limit):\n",
    "      \n",
    "      #iterate each instance\n",
    "      mini_batch_count = 0\n",
    "      for instance in range(len_data):\n",
    "            \n",
    "        # From input layer to hidden layer\n",
    "        ## iterate every hidden layer to fill the values\n",
    "        for hidden_unit in range(self.n_hidden):\n",
    "          ### calculate the net input\n",
    "          self.pre_activation_H[hidden_unit] = self.calculate_net_ItoH(instance, hidden_unit)\n",
    "          ### calculate the activated value\n",
    "          self.post_activation_H[hidden_unit] = self.activation(self.pre_activation_H[hidden_unit])\n",
    "\n",
    "        # From hidden layer to output layer\n",
    "        for output_unit in range(self.n_outputs):\n",
    "          ### calculate the net input\n",
    "          self.pre_activation_O[output_unit] = self.calculate_net_HtoO(output_unit)\n",
    "          ### calculate the activated value\n",
    "          self.post_activation_O[output_unit] = self.activation(self.pre_activation_O[output_unit])\n",
    "      \n",
    "        #for debug\n",
    "        if(epoch_limit == 1):\n",
    "            print('INSTANCE:', instance )\n",
    "            print('WEIGHTS\\n', self.weights_ItoH, '\\n', self.weights_HtoO)\n",
    "            print('OUTPUTS\\n', self.post_activation_H, '\\n', self.post_activation_O)\n",
    "\n",
    "        # Backpropagation\n",
    "        ## if already at minibatch limit or at the last instance, update the weight \n",
    "        if((mini_batch_count == mini_batch_limit) or (instance == len_data - 1)):\n",
    "          #update weight - hidden to output\n",
    "          self.weights_HtoO = np.add(self.weights_HtoO, self.dweights_HtoO)\n",
    "          \n",
    "          #update weight - input to hidden\n",
    "          self.weights_ItoH = np.add(self.weights_ItoH, self.dweights_ItoH)\n",
    "\n",
    "          mini_batch_count = 0\n",
    "        \n",
    "        ## if below minibatch limit, update delta-weight\n",
    "        else:\n",
    "          ### update delta-weight from output\n",
    "          for hidden_unit in range(self.n_hidden + 1): # (+1 accomodating bias)\n",
    "            for output_unit in range(self.n_outputs):\n",
    "              #### (Minus sign merged). Formula: (target_ok - out_ok) * out_ok * (1 - out_ok) * out_hj\n",
    "              target_o = self.target[instance][output_unit]\n",
    "              out_o = self.post_activation_O[output_unit]\n",
    "              \n",
    "              ##### calculating weight of bias\n",
    "              if (hidden_unit == self.n_hidden): \n",
    "                out_h = self.bias\n",
    "              ##### calculating weight of activated hidden unit\n",
    "              else:\n",
    "                out_h = self.post_activation_H[hidden_unit]\n",
    "\n",
    "              self.error_O[output_unit] = (target_o - out_o) * out_o * (1 - out_o)\n",
    "              self.dweights_HtoO[hidden_unit][output_unit] += self.error_O[output_unit] * out_h\n",
    "\n",
    "          ### update delta-weight from hidden layer\n",
    "          for input_unit in range(self.n_inputs + 1): # (+1 accomodating bias)\n",
    "            for hidden_unit in range(self.n_hidden):\n",
    "              #### Formula: sigma_ok(error_o * w_ho) * out_hj * (1 - out_hj) * input_i\n",
    "              sigma_err_output = np.dot(self.error_O, self.weights_HtoO[hidden_unit,:])\n",
    "              out_h = self.post_activation_H[hidden_unit]\n",
    "\n",
    "              ##### calculating weight of bias\n",
    "              if(input_unit == self.n_inputs): \n",
    "                input_i = self.bias\n",
    "              ##### calculating weight of input unit\n",
    "              else:\n",
    "                input_i = self.data[instance, input_unit] \n",
    "              \n",
    "              self.error_H[hidden_unit] = sigma_err_output * out_h * (1 - out_h)\n",
    "              self.dweights_ItoH[input_unit][hidden_unit] += self.error_H[hidden_unit] * input_i\n",
    "\n",
    "          mini_batch_count += 1\n",
    "        \n",
    "\n",
    "  def predict(self, data):\n",
    "    self.data = data\n",
    "    result = np.zeros(len(data))\n",
    "    #iterate each instance\n",
    "    for instance in range(len(data)):      \n",
    "      ## iterate every hidden layer to fill the values\n",
    "      for hidden_unit in range(self.n_hidden):\n",
    "        ### calculate the net input\n",
    "        self.pre_activation_H[hidden_unit] = self.calculate_net_ItoH(instance, hidden_unit)\n",
    "        ### calculate the activated value\n",
    "        self.post_activation_H[hidden_unit] = self.activation(self.pre_activation_H[hidden_unit])\n",
    "\n",
    "      max_value = 0\n",
    "      max_index = -1 \n",
    "      # From hidden layer to output layer\n",
    "      for output_unit in range(self.n_outputs):\n",
    "        ### calculate the net input\n",
    "        self.pre_activation_O[output_unit] = self.calculate_net_HtoO(output_unit)\n",
    "        ### calculate the activated value\n",
    "        self.post_activation_O[output_unit] = self.activation(self.pre_activation_O[output_unit])\n",
    "        if(self.post_activation_O[output_unit] >= max_value ):\n",
    "          max_value = self.post_activation_O[output_unit]\n",
    "          max_index = output_unit\n",
    "      print(self.post_activation_O)\n",
    "      print('instance no:', instance, 'prediction result:', max_index)\n",
    "      result = np.append(result, max_index)\n",
    "    \n",
    "    return result\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training\n",
    "print('Data Iris')\n",
    "load, target = load_iris(return_X_y=True)\n",
    "iris_data = pd.DataFrame(load, columns=['sepal_length', 'sepal_width', 'petal_length', 'petal_width'])\n",
    "iris_data['label'] = pd.Series(target)\n",
    "\n",
    "shuffled_data = iris_data.sample(frac=1)\n",
    "train_X = shuffled_data.drop('label',axis=1,inplace=False).values\n",
    "train_y = shuffled_data['label'].values\n",
    "\n",
    "net = Network(4, 4)\n",
    "net.fit(train_X, train_y, epoch_limit=10)\n",
    "\n",
    "#Testing\n",
    "shuffled_data = iris_data.sample(n=20)\n",
    "test_X = shuffled_data.drop('label',axis=1,inplace=False).values\n",
    "test_y = shuffled_data['label'].values\n",
    "\n",
    "result = net.predict(test_X)\n",
    "print(\"Pred Result\\n\", result, sep='')\n",
    "print(\"Original Data\\n\", test_y, sep='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3.11740136 8.27094941 5.9164251 ] [3.51638315 5.36092207 1.32702712]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "63.1531494133005"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "o = np.random.uniform(1, 10, 3)\n",
    "j = np.random.uniform(1, 10, 3)\n",
    "print(o, j)\n",
    "np.dot(o, j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.random.uniform(1,1,(3,5))\n",
    "b = np.random.uniform(1,10,(3,5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1.]] \n",
      "\n",
      "[[4.34001615 8.84582444 5.36002125 8.67189147 5.18367231]\n",
      " [8.42507471 4.31171515 7.97301027 6.15631649 9.27238015]\n",
      " [7.58748903 6.40231917 8.78151948 7.05457473 4.83981053]] \n",
      "\n",
      "[[ 5.34001615  9.84582444  6.36002125  9.67189147  6.18367231]\n",
      " [ 9.42507471  5.31171515  8.97301027  7.15631649 10.27238015]\n",
      " [ 8.58748903  7.40231917  9.78151948  8.05457473  5.83981053]]\n"
     ]
    }
   ],
   "source": [
    "print(a, '\\n')\n",
    "print(b, '\\n')\n",
    "print(np.add(a,b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1. 1. 1. 1. 1.]\n",
      "[1. 1. 1. 1. 1. 5.]\n"
     ]
    }
   ],
   "source": [
    "a = np.random.uniform(1,1,5)\n",
    "print(a)\n",
    "a = np.append(a, 5)\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
